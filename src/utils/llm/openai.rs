use std::collections::HashMap;
use std::fmt::Debug;
use anyhow::anyhow;
use async_openai::Client;
use async_openai::config::Config;
use async_openai::error::OpenAIError;
use async_openai::types::{ChatChoice, ChatCompletionRequestMessage, CreateChatCompletionRequest, CreateChatCompletionResponse, Role, Stop};
use crate::utils::JsonMap;

/// Configuration for OpenAI LLM in a conversation setting. Partially copied from [async_openai::types::CreateChatCompletionRequest].
#[derive(Debug, Clone)]
pub struct ConversationConfig {
    /// ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.
    pub model: String,

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    ///
    /// We generally recommend altering this or `top_p` but not both.

    pub temperature: Option<f32>, // min: 0, max: 2, default: 1,

    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    ///
    ///  We generally recommend altering this or `temperature` but not both.

    pub top_p: Option<f32>, // min: 0, max: 1, default: 1

    /// How many chat completion choices to generate for each input message.

    pub n: Option<u8>, // min:1, max: 128, default: 1


    /// Up to 4 sequences where the API will stop generating further tokens.

    pub stop: Option<Stop>,

    /// The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).

    pub max_tokens: Option<u16>, // default: inf

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    ///
    /// [See more information about frequency and presence penalties.](https://platform.openai.com/docs/api-reference/parameter-details)

    pub presence_penalty: Option<f32>, // min: -2.0, max: 2.0, default 0

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    ///
    /// [See more information about frequency and presence penalties.](https://platform.openai.com/docs/api-reference/parameter-details)

    pub frequency_penalty: Option<f32>, // min: -2.0, max: 2.0, default: 0

    /// Modify the likelihood of specified tokens appearing in the completion.
    ///
    /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

    pub logit_bias: Option<HashMap<String, serde_json::Value>>, // default: null

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).

    pub user: Option<String>,
}

impl Default for ConversationConfig {
    fn default() -> Self {
        Self {
            model: "gpt-3.5-turbo".to_string(),
            temperature: None,
            top_p: None,
            n: None,
            stop: None,
            max_tokens: None,
            presence_penalty: None,
            frequency_penalty: None,
            logit_bias: None,
            user: None,
        }
    }
}

/// A message in a conversation with optional metadata.
#[derive(Debug, Clone)]
pub struct ChatMsg {
    pub content: ChatCompletionRequestMessage,
    pub metadata: Option<JsonMap>,
}

/// A conversation with OpenAI LLM.
/// FIXME: add function calling support.
pub struct Conversation<ClientConfig: Config> {
    pub client: Client<ClientConfig>,
    pub configs: ConversationConfig,
    pub history: Vec<ChatMsg>,
}

impl<ClientConfig: Config> Conversation<ClientConfig> {
    /// Create a new conversation with OpenAI LLM.
    pub fn new(client: Client<ClientConfig>, configs: ConversationConfig) -> Self {
        Self {
            client,
            configs,
            history: Vec::new(),
        }
    }

    /// Insert a message into the conversation history.
    pub fn insert_history(&mut self, content: impl Into<String>, role: Role, metadata: Option<JsonMap>) {
        let chat_msg = ChatCompletionRequestMessage {
            role,
            content: Some(content.into()),
            name: None,
            function_call: None,
        };
        self.history.push(ChatMsg {
            content: chat_msg,
            metadata,
        });
    }

    /// Commit a chat request to OpenAI LLM with the current conversation history.
    pub async fn commit_request(&self) -> Result<CreateChatCompletionResponse, OpenAIError> {
        let config = self.configs.clone();
        let request = CreateChatCompletionRequest {
            model: config.model,
            messages: self.history.iter().map(|msg| msg.content.clone()).collect(),
            functions: None,
            function_call: None,
            temperature: config.temperature,
            top_p: config.top_p,
            n: config.n,
            stream: None,
            stop: config.stop,
            max_tokens: config.max_tokens,
            presence_penalty: config.presence_penalty,
            frequency_penalty: config.frequency_penalty,
            logit_bias: config.logit_bias,
            user: config.user,
        };
        self.client.chat().create(request).await
    }

    /// Insert a message into the conversation history and commit a chat request to OpenAI LLM.
    pub async fn query_raw(&mut self, content: impl Into<String>, role: Role, metadata: Option<JsonMap>) -> Result<CreateChatCompletionResponse, OpenAIError> {
        self.insert_history(content, role, metadata);
        self.commit_request().await
    }

    /// Insert a message into the conversation history and commit a chat request to OpenAI LLM.
    pub async fn query(&mut self, content: impl Into<String>, role: Role, metadata: Option<JsonMap>) -> Result<Vec<ChatChoice>, OpenAIError> {
        self.query_raw(content, role, metadata).await.and_then(|response| Ok(response.choices))
    }

    /// Insert a message into the conversation history and commit a chat request to OpenAI LLM.
    pub async fn chat(&mut self, content: impl Into<String>, role: Role, metadata: Option<JsonMap>) -> anyhow::Result<String> {
        let response = self.query(content, role, metadata).await?;
        response.first().unwrap().message.content.clone().ok_or_else(|| anyhow!("No content in response"))
    }
}