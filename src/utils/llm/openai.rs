use std::collections::HashMap;
use std::fmt::{Debug, Formatter, Display};
use serde::{Deserialize, Serialize};
use async_openai::Client;
use async_openai::config::Config;
use async_openai::error::OpenAIError;
use async_openai::types::{ChatCompletionFunctionCall, ChatCompletionFunctions, ChatCompletionRequestMessage, ChatCompletionResponseStream, ChatCompletionStreamResponseDelta, CreateChatCompletionRequest, CreateChatCompletionResponse, FunctionCall, Role, Stop};
use crate::utils::JsonMap;
use crate::utils::token::tiktoken::{MODEL_TO_MAX_TOKENS, Tiktoken};

/// Configuration for OpenAI LLM in a conversation setting. Partially copied from [async_openai::types::CreateChatCompletionRequest].
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConversationConfig {
    /// ID of the model to use.
    /// See the [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
    pub model: String,

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random,
    /// while lower values like 0.2 will make it more focused and deterministic.
    ///
    /// We generally recommend altering this or `top_p` but not both.
    pub temperature: Option<f32>, // min: 0, max: 2, default: 1,

    /// An alternative to sampling with temperature, called nucleus sampling,
    /// where the model considers the results of the tokens with top_p probability mass.
    /// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
    ///
    ///  We generally recommend altering this or `temperature` but not both.
    pub top_p: Option<f32>, // min: 0, max: 1, default: 1

    /// How many chat completion choices to generate for each input message.
    pub n: Option<u8>, // min:1, max: 128, default: 1


    /// Up to 4 sequences where the API will stop generating further tokens.
    pub stop: Option<Stop>,

    /// The maximum number of [tokens](https://platform.openai.com/tokenizer) to generate in the chat completion.
    ///
    /// The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) for counting tokens.
    pub max_tokens: Option<u16>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    ///
    /// [See more information about frequency and presence penalties.](https://platform.openai.com/docs/api-reference/parameter-details)
    pub presence_penalty: Option<f32>, // min: -2.0, max: 2.0, default 0

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    ///
    /// [See more information about frequency and presence penalties.](https://platform.openai.com/docs/api-reference/parameter-details)
    pub frequency_penalty: Option<f32>, // min: -2.0, max: 2.0, default: 0

    /// Modify the likelihood of specified tokens appearing in the completion.
    ///
    /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
    /// Mathematically, the bias is added to the logits generated by the model prior to sampling.
    /// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
    /// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    pub logit_bias: Option<HashMap<String, serde_json::Value>>, // default: null

    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).
    pub user: Option<String>,
}

impl Default for ConversationConfig {
    fn default() -> Self {
        Self {
            model: "gpt-3.5-turbo".to_string(),
            temperature: None,
            top_p: None,
            n: None,
            stop: None,
            max_tokens: None,
            presence_penalty: None,
            frequency_penalty: None,
            logit_bias: None,
            user: None,
        }
    }
}

/// A message in a conversation with optional metadata.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChatMsg {
    pub msg: ChatCompletionRequestMessage,
    pub metadata: Option<JsonMap>,
}

impl Display for ChatMsg {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", serde_json::to_string_pretty(&self.msg).unwrap())
    }
}

impl ChatMsg {
    pub fn merge_delta(&mut self, delta: &ChatCompletionStreamResponseDelta) {
        const END_OP: Option<()> = Some(()); // To stop compiler complaining type error
        // if we have a function call delta, we need to update the function call
        delta.function_call
            .as_ref()
            .and_then(|fn_call_delta| {
                // if the container message already has a function call, we need to update the function call
                if let Some(fn_call) = self.msg.function_call.as_mut() {
                    fn_call_delta.name
                        .as_ref()
                        .and_then(|fn_name| {
                            fn_call.name = fn_name.clone();
                            END_OP
                        });
                    fn_call_delta.arguments
                        .as_ref()
                        .and_then(|fn_args| {
                            fn_call.arguments.push_str(fn_args);
                            END_OP
                        });
                } else {
                    // if the container message does not have a function call, we need to create one
                    self.msg.function_call = Some(FunctionCall {
                        name: fn_call_delta.name.as_ref().map_or_else(String::new, Clone::clone),
                        arguments: fn_call_delta.arguments.as_ref().map_or_else(String::new, Clone::clone),
                    });
                }
                END_OP
            });
        // if we have a content delta, we need to update the content
        delta.content
            .as_ref()
            .and_then(|content_delta| {
                // if the container message already has a content, we need to update the content
                if let Some(content) = self.msg.content.as_mut() {
                    content.push_str(content_delta.as_str());
                } else {
                    // if the container message does not have a content, we need to create one
                    self.msg.content = Some(content_delta.clone());
                }
                END_OP
            });
    }
}

/// A conversation with OpenAI LLM.
#[derive(Clone)]
pub struct Conversation<ClientConfig: Config + Debug> {
    pub client: Client<ClientConfig>,
    pub configs: ConversationConfig,
    pub history: Vec<ChatMsg>,
    pub auto_truncate_history: bool,
    pub tiktoken: Tiktoken,
}

impl<ClientConfig: Config + Debug> Display for Conversation<ClientConfig> {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        write!(f, "{}", serde_json::to_string_pretty(&self.history).unwrap())
    }
}

impl<ClientConfig: Config + Debug> Debug for Conversation<ClientConfig> {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        write!(f, r#"
client: {:?}
configs: {}
history: {}
auto_truncate_history: {}
"#, self.client, serde_json::to_string_pretty(&self.configs).unwrap(), serde_json::to_string_pretty(&self.history).unwrap(), self.auto_truncate_history)
    }
}

impl<ClientConfig: Config + Debug> Conversation<ClientConfig> {
    /// Create a new conversation with OpenAI LLM.
    pub fn new(client: Client<ClientConfig>,
               configs: ConversationConfig,
               auto_truncate_history: bool) -> Self {
        let tiktoken = Tiktoken::new(configs.model.clone()).unwrap();
        Self {
            client,
            configs,
            history: Vec::new(),
            auto_truncate_history,
            tiktoken,
        }
    }

    /// Count the number of tokens in the conversation history.
    pub fn count_tokens_history(&self) -> usize {
        self.history.iter().map(|msg| self.tiktoken.count_msg_token(&msg.msg)).sum()
    }

    /// Insert a message into the conversation history.
    pub fn insert_history(&mut self,
                          message: ChatCompletionRequestMessage,
                          metadata: Option<JsonMap>) {
        self.history.push(ChatMsg {
            msg: message,
            metadata,
        });
        if self.auto_truncate_history {
            self.truncate_history();
        }
    }

    #[inline]
    fn create_chat_request(&self,
                           functions: Option<Vec<ChatCompletionFunctions>>,
                           function_call: Option<ChatCompletionFunctionCall>,
                           stream: bool) -> CreateChatCompletionRequest {
        let config = self.configs.clone();
        CreateChatCompletionRequest {
            model: config.model,
            messages: self.history.iter().map(|msg| msg.msg.clone()).collect(),
            functions,
            function_call,
            temperature: config.temperature,
            top_p: config.top_p,
            n: config.n,
            stream: if stream { Some(true) } else { None },
            stop: config.stop,
            max_tokens: config.max_tokens,
            presence_penalty: config.presence_penalty,
            frequency_penalty: config.frequency_penalty,
            logit_bias: config.logit_bias,
            user: config.user,
        }
    }


    /// Commit a chat request to OpenAI LLM with the current conversation history.
    pub async fn query_with_history(&self,
                                    functions: Option<Vec<ChatCompletionFunctions>>,
                                    function_call: Option<ChatCompletionFunctionCall>) -> Result<CreateChatCompletionResponse, OpenAIError> {
        let chat_request = self.create_chat_request(functions, function_call, false);
        self.client.chat().create(chat_request).await
    }

    /// Commit a chat request to OpenAI LLM with the current conversation history.
    /// Returns a stream
    pub async fn query_and_stream_with_history(&self,
                                               functions: Option<Vec<ChatCompletionFunctions>>,
                                               function_call: Option<ChatCompletionFunctionCall>) -> Result<ChatCompletionResponseStream, OpenAIError> {
        let chat_request = self.create_chat_request(functions, function_call, true);
        self.client.chat().create_stream(chat_request).await
    }

    pub fn truncate_history(&mut self) {
        let mut max_tokens = *MODEL_TO_MAX_TOKENS.get(self.configs.model.as_str()).unwrap();
        let sys_prompt = self.history.first().and_then(|chat_msg| {
            if chat_msg.msg.role == Role::System {
                max_tokens -= self.tiktoken.count_msg_token(&chat_msg.msg);
                Some(chat_msg)
            } else {
                None
            }
        });
        let truncate_start_idx = self.tiktoken.get_truncate_start_idx(&self.history.iter().map(|chat_msg| chat_msg.msg.clone()).collect(), max_tokens);
        if truncate_start_idx > 0 {
            if let Some(sys_prompt) = sys_prompt {
                let mut new_history = Vec::with_capacity(self.history.len() - truncate_start_idx + 1);
                new_history.push(sys_prompt.clone());
                new_history.extend_from_slice(&self.history[truncate_start_idx..]);
                self.history = new_history;
            } else {
                self.history = self.history[truncate_start_idx..].to_vec();
            }
        }
    }
}